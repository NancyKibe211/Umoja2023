{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1679183119652,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "0kYQU1igz8PH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c11a4cbb"
   },
   "source": [
    "# Introduction to training on the Rubik's Cube \n",
    "<a href=\"https://githubtocolab.com/adsodemelk/umoja23/blob/main/Rubik's%20Cube%20walkthrough%20colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a68bc156"
   },
   "source": [
    "This notebook will walk through some examples of how to run training with the model as set up currently, as well as how to make modifications and explore your own solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1679183119654,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "MS5q8Z3uG8PW"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#os.chdir(\"/content/drive/MyDrive/umojahack-africa-2023-advanced-challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1679183119656,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "zGlQyASpG7Pj"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/adsodemelk/umoja23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21892,
     "status": "ok",
     "timestamp": 1679183141520,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "c7896bfe",
    "outputId": "114be315-94f4-414e-ea03-11d28d14e11c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#Download the necesary files from a public bucket\n",
    "#!gsutil cp -r gs://umoja23/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210438,
     "status": "ok",
     "timestamp": 1679183351938,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "48668162",
    "outputId": "0eb3833c-5a73-42d8-bf1b-6cce7303a5e0"
   },
   "outputs": [],
   "source": [
    "# Istall requirements\n",
    "#!pip install -Ur umoja23/requirements/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f848219e"
   },
   "source": [
    "## Imports\n",
    "Here we will be importing the moduls we are using. \n",
    "\n",
    "\n",
    "**Note** : If you face issues importing numpy. Try restarting the runtime (go to Runtime -> Restart runtime) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "executionInfo": {
     "elapsed": 24746,
     "status": "error",
     "timestamp": 1679183376663,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "fd5e7f6c",
    "outputId": "aa5d6914-d55a-42c9-e6a2-eb121ff08b25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add installed package to the path\n",
    "import os, sys \n",
    "sys.path.append(os.path.abspath(\"./umoja23\"))\n",
    "\n",
    "\n",
    "# Environment\n",
    "from rubiks_cube.env import RubiksCube, create_flattened_env\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training\n",
    "\n",
    "from umoja23.training.example import main_training\n",
    "from umoja23.training.configs import CUSTOM_MODEL_CONFIG\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from ray.rllib.algorithms import AlgorithmConfig\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from umoja23.training.configs import get_config, MEDIUM_ENV_CONFIG\n",
    "from umoja23.training.registry import register, _ray\n",
    "\n",
    "from umoja23.evaluation.seeds import PUBLIC_SEEDS\n",
    "from umoja23.evaluation.generate_rollout import main_rollout\n",
    "from evaluation.validate_rollout import main_validation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/home/duamelo/ray_results/PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acf7f6ee"
   },
   "source": [
    "## Introduction to the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2b16bd8"
   },
   "source": [
    "The Rubik's Cube is likely a very familiar puzzle to everyone! In case you would like to read more about its history or some of the maths and mechanics behind it, the Wikipedia page is a good starting point: https://en.wikipedia.org/wiki/Rubik%27s_Cube\n",
    "\n",
    "We present here an implementation in python (numpy) that allows you to explore how reinforcement learning can be applied to the problem.\n",
    "\n",
    "First let's instantiate and walk through some of the basic mechanics of the environment. Feel free to consult rubiks_cube/env.py to see details of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "aborted",
     "timestamp": 1679183376665,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "EmvQwhQo0QHq"
   },
   "outputs": [],
   "source": [
    "STEP_LIMIT, ITERS, NUM_SCRAMBLES_ON_RESET = 100, 100, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "aborted",
     "timestamp": 1679183376666,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "742654bb"
   },
   "outputs": [],
   "source": [
    "env = RubiksCube(step_limit=STEP_LIMIT, \n",
    "                 reward_function_type=\"sparse\", \n",
    "                 num_scrambles_on_reset=NUM_SCRAMBLES_ON_RESET)\n",
    "\n",
    "assert isinstance(env, gym.Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d21532b"
   },
   "source": [
    "The observation space consists of\n",
    "\n",
    "1: the cube, with ids indicating the colour each sticker. There are 6 faces, each containing 3x3 stickers;\n",
    "\n",
    "2: the step_count, which starts at 0 and increments by 1 on every turn taken, until the environment step_limit is hit (at which point the episode must end).\n",
    "\n",
    "The action space consists of a face to turn, and an amount. Faces can be 6 possible values, and amounts can be 3, corresponding to (in order) clockwise, anti-clockwise, and a half turn.\n",
    "\n",
    "Convention:\n",
    "\n",
    "0 = up face\n",
    "\n",
    "1 = front face\n",
    "\n",
    "2 = right face\n",
    "\n",
    "3 = back face\n",
    "\n",
    "4 = left face\n",
    "\n",
    "5 = down face\n",
    "\n",
    "All read in reading order when looking directly at face\n",
    "\n",
    "To look directly at the faces:\n",
    "\n",
    "UP: LEFT face on the left and BACK face pointing up\n",
    "\n",
    "FRONT: LEFT face on the left and UP face pointing up\n",
    "\n",
    "RIGHT: FRONT face on the left and UP face pointing up\n",
    "\n",
    "BACK: RIGHT face on the left and UP face pointing up\n",
    "\n",
    "LEFT: BACK face on the left and UP face pointing up\n",
    "\n",
    "DOWN: LEFT face on the left and FRONT face pointing up\n",
    "\n",
    "Turn amounts (eg clockwise) are when looking directly at the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "aborted",
     "timestamp": 1679183376667,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "3e869aa7"
   },
   "outputs": [],
   "source": [
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "aborted",
     "timestamp": 1679183376668,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "4dc30a6a"
   },
   "outputs": [],
   "source": [
    "# Render the environment\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36a9cede"
   },
   "source": [
    "As mentioned, an action consists of specifying a face and an amount to turn it by. We can visualise the impact of an action by using the rendering above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376668,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "e9513d6d"
   },
   "outputs": [],
   "source": [
    "# Don't scramble the cube so it's easier to understand what's happening\n",
    "env = RubiksCube(step_limit=STEP_LIMIT,\n",
    "                 reward_function_type=\"sparse\", \n",
    "                 num_scrambles_on_reset=0)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376669,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "03791ad4"
   },
   "outputs": [],
   "source": [
    "# Turn the UP face clockwise (by 90 degrees)\n",
    "action = (0, 0)\n",
    "obs, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e923adb"
   },
   "source": [
    "By default, the reward function is +1 if the cube is solved, otherwise 0. This is specified by passing reward_function_type to the environment constructor. You can implement your own custom reward function if you think that doing so would better incentivise the agent to learn how to solve the cube! However, be aware that the final evaluation will be purely on how often the cube is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1679183376669,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "33585280"
   },
   "outputs": [],
   "source": [
    "print(f\"Reward before solving: {reward}\")\n",
    "\n",
    "# Invert the above action\n",
    "action = (0, 1)\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(f\"Reward after solving: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbc561ce"
   },
   "source": [
    "The episode can finish for one of two reasons:\n",
    "\n",
    "1: The cube is solved\n",
    "\n",
    "2: The step limit is hit. In this case, all rewards throughout the episode will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376670,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "19ff7617"
   },
   "outputs": [],
   "source": [
    "print(f\"Above environment has finished? {done}\")\n",
    "\n",
    "env = RubiksCube(step_limit=STEP_LIMIT,\n",
    "                 reward_function_type=\"sparse\", \n",
    "                 num_scrambles_on_reset=NUM_SCRAMBLES_ON_RESET)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"New environment has finished? {done}. Reward obtained={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b91a7d94"
   },
   "source": [
    "For some models it might be more helpful to \"flatten\" the action space, meaning to have a single integer between 0 and 17 rather than a tuple of integers. For this purpose you can use the already implemented wrapper. To get this behaviour in models below, you will typically want to set FLATTEN_ACTIONS to True in training/configs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376670,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "77359d83"
   },
   "outputs": [],
   "source": [
    "env = create_flattened_env(dict(step_limit=STEP_LIMIT,\n",
    "                                reward_function_type=\"sparse\", \n",
    "                                num_scrambles_on_reset=NUM_SCRAMBLES_ON_RESET))\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "848a8883"
   },
   "source": [
    "Finally, you can combine multiple frames together to create an animation of your Rubik's Cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376671,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "695148aa"
   },
   "outputs": [],
   "source": [
    "env = RubiksCube(step_limit=STEP_LIMIT,\n",
    "                 reward_function_type=\"sparse\",\n",
    "                 num_scrambles_on_reset=100)\n",
    "obs = env.reset()\n",
    "cubes = [obs[\"cube\"].copy()]\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    cubes.append(obs[\"cube\"].copy())\n",
    "\n",
    "anim = env.animation(cubes)\n",
    "anim.save(\"cube.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d0e96cc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f5e6a34"
   },
   "source": [
    "The algorithm implemented here is Proximal Policy Optimization (PPO), which is an on-policy reinforcement learning algorithm developed by OpenAI. For more information please consult their page: https://openai.com/research/openai-baselines-ppo. Note that one could equally use other algorithms such as DQN and you encouraged to experiment with this, though it may involve changing other parts of the code such as evaluation/generate_rollout.py.\n",
    "\n",
    "Fundamentally, a PPO agent should implement a policy network and a value network. The policy network maps observations to a parametrisation of the policy that is used to select the next action, whereas the value network maps observations to a single number which is supposed to estimate the expected discounted future reward, and is used to stabilise the training. The model implemented here by default shares most of the parameters between these two networks, though you are encouraged to experiment with different model architectures.\n",
    "\n",
    "To understand the architecture of the model, examine the model class in training/PPO_models.py. By default it will use FactorisedPPOModel, which takes as input the cube and step count from the observation, and produces an encoding of the observation, which will be a fixed size vector of size given by the final entry of dense_layer_dims in the model_config (defined in training.configs.CUSTOM_MODEL_CONFIG). This encoding is projected to a single number for predicting the value. As for the policy, we take advantage of the factorised/conditional structure of the action space to produce first logits for the decision of which face to turn - the model's face_selection_model. Finally, we compute a conditional distribution by concatenating a sampled value of the selected face (encoded as a one-hot) to the encoding and projecting to give logits for the decision of how much to turn the face - the model's cube_movement_amount_selection_model. This model must be used in combination with the FactorisedActionDistribution.\n",
    "\n",
    "Note that if one instead flattens the action space this conditional structure will no longer be present and so one should use a different model like FlatPPOModel, and will not need to specify a custom action distribution (since by default PPO is expecting the model to output logits for a flat action space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cfe7e3d"
   },
   "source": [
    "### Training your first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e888c0a"
   },
   "source": [
    "The main training script will write a checkpoint (learned model weights and associated metadata) that you will later be able to use to evaluate training on unseen instances.\n",
    "\n",
    "This script can also be run from the command line; the following command will execute the same code as the python in the cell below:\n",
    "\n",
    "python training/example.py --step_limit 10 --reward_function_type sparse --num_scrambles_on_reset 2 --agent_name PPO --num_iterations 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1679183376671,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "1a95eed7"
   },
   "outputs": [],
   "source": [
    "main_training(\n",
    "    step_limit=STEP_LIMIT,\n",
    "    reward_function_type=\"sparse\",\n",
    "    num_scrambles_on_reset=NUM_SCRAMBLES_ON_RESET,\n",
    "    model_config=CUSTOM_MODEL_CONFIG,\n",
    "    agent_name=\"PPO\",\n",
    "    num_iterations=ITERS,\n",
    "    restore_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bd7ac9c"
   },
   "source": [
    "As you can see there is quite a lot of output! The main highlights are the path to the checkpoint (here the directory is ~/ray_results/PPO/PPO_rubiks_cube_env_f42af_00000_0_2023-02-28_14-55-34), a summary of the model used (which you can override by modifying training/PPO_models.py) and the per iteration results. The latter is a series of metrics summarising what happened during the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fc98045"
   },
   "source": [
    "You might find it easier to view these in tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1679183376672,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "616d62f8"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1679183376672,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "HpzIhn_YbyY7"
   },
   "outputs": [],
   "source": [
    "log_dir = sorted([x for x in os.listdir(\"/home/duamelo/ray_results/PPO/\") if x.startswith(\"PPO\")])[-1]\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1679183376673,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "kQlcJv9kBsAq"
   },
   "outputs": [],
   "source": [
    "chk_pt = sorted([x for x in os.listdir(f\"/home/duamelo/ray_results/PPO/{log_dir}\") if x.startswith(\"checkpoint\")])[-1]\n",
    "chk_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1679183376673,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "fu-VeseiB-11"
   },
   "outputs": [],
   "source": [
    "def get_dir(root=False):\n",
    "  log_dir = sorted([x for x in os.listdir(\"/home/duamelo/ray_results/PPO/\") if x.startswith(\"PPO\")])[-1]\n",
    "  chk_pt = sorted([x for x in os.listdir(f\"/home/duamelo/ray_results/PPO/{log_dir}\") if x.startswith(\"checkpoint\")])[-1]\n",
    "  if root:\n",
    "    return f\"/home/duamelo/ray_results/PPO/{log_dir}\"    \n",
    "  return f\"/home/duamelo/ray_results/PPO/{log_dir}/{chk_pt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1679183376674,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "f2e637ec"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir ~/ray_results/PPO/PPO_rubiks_cube_env_f62df_00000_0_2023-03-18_23-16-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ed16b41"
   },
   "source": [
    "To see some basic documentation of how to use the main training script, run in terminal:\n",
    "\n",
    "python training/example.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86a08922"
   },
   "source": [
    "### Training your second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e542fe73"
   },
   "source": [
    "The library allows you to restore from a checkpoint and continue training, without having to redo the first stages of training. This could be useful in various situations, for example if the training crashed, or if you want to start training on easier environments before progressing to more difficult examples as the agent gets better (curriculum learning). Note that this will only work if the model can stay the same throughout.\n",
    "\n",
    "Let's restore the checkpoint generated above and continue training on a harder environment. We will increase the number of scrambles done to the cube on reset. Note that we must increase the number of training iterations as otherwise there is nothing to do. The results of this training will be written to a new directory.\n",
    "\n",
    "This is equivalent to the following command:\n",
    "\n",
    "python training/example.py --step_limit 10 --reward_function_type sparse --num_scrambles_on_reset 5 --agent_name PPO --num_iterations 4 --restore_path ~/ray_results/PPO/PPO_rubiks_cube_env_f42af_00000_0_2023-02-28_14-55-34/checkpoint_000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1679183376674,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "BEjNyQQJOxKT"
   },
   "outputs": [],
   "source": [
    "os.listdir(f\"{get_dir(True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "aborted",
     "timestamp": 1679183376674,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "8ed9b41b"
   },
   "outputs": [],
   "source": [
    "main_training(\n",
    "    step_limit=STEP_LIMIT,\n",
    "    reward_function_type=\"sparse\",\n",
    "    num_scrambles_on_reset=NUM_SCRAMBLES_ON_RESET,\n",
    "    model_config=CUSTOM_MODEL_CONFIG,\n",
    "    agent_name=\"PPO\",\n",
    "    num_iterations=ITERS,\n",
    "    restore_path=f\"{get_dir()}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3494a509"
   },
   "source": [
    "Note that only 2 additional iterations of training were performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4184d73"
   },
   "source": [
    "There is a lot more customisablity available for rllib, feel free to consult documentation at https://docs.ray.io/en/latest/rllib/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b4688ac"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a8c1153"
   },
   "source": [
    "The final stage to is to produce a set of rollouts - that is sequences of actions and new observations that one obtains by repeatedly inferring from a model and taking the corresponding action in the environment. Included below is a simple set of commands that are likely to be useful if you would like to investigate the decisions that your model is making in particular situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "aborted",
     "timestamp": 1679183376675,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "6adf83ed"
   },
   "outputs": [],
   "source": [
    "config = get_config(\n",
    "    env_config=MEDIUM_ENV_CONFIG,\n",
    "    model_config=CUSTOM_MODEL_CONFIG,\n",
    "    agent_name=\"PPO\",\n",
    ")\n",
    "register(agent_name=\"PPO\")\n",
    "with _ray():\n",
    "    agent = PPO(AlgorithmConfig.from_dict(config))\n",
    "    agent.restore(f\"{get_dir()}\")\n",
    "    env = RubiksCube(**MEDIUM_ENV_CONFIG)\n",
    "    obs = env.reset()\n",
    "    # Exploration means to sample from the parametrised distribution. Setting it to false picks the modal action\n",
    "    action = agent.compute_single_action(observation=obs, explore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376675,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "208602ec"
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376675,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "14606439"
   },
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376676,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "01ae7d93"
   },
   "outputs": [],
   "source": [
    "print(f\"Agent is choosing action {action}\")\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376676,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "def7c234"
   },
   "outputs": [],
   "source": [
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5cd0420"
   },
   "source": [
    "For a more systematic way of doing this, there is a script which will run this automatically on a collection of increasingly difficult environment configurations and write the result to a file. To use this script, run:\n",
    "\n",
    "python evaluation/generate_rollout.py\n",
    "\n",
    "while specifying command line arguments checkpoint_path (indicating the path to restore from if using a trained model), the results_path (where to write the results of the inference to), and the agent_name (can leave blank if using PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376677,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "dnXmLq3tNzcN"
   },
   "outputs": [],
   "source": [
    "# PUBLIC_SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376677,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "b8f88508"
   },
   "outputs": [],
   "source": [
    "# Example on a smaller set of seeds\n",
    "/home/duamelo/home/duamelo\n",
    "main_rollout(seeds=PUBLIC_SEEDS,\n",
    "             checkpoint_path=f\"{get_dir()}\", \n",
    "             results_path=f\"sample_results_steps_{STEP_LIMIT}_iter_{ITERS}_dl_dims_{'_'.join([str(x) for x in CUSTOM_MODEL_CONFIG['dense_layer_dims']])}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6a18a4"
   },
   "source": [
    "Finally, you can run the script python evaluation/validate_rollout.py to make sure that the results are given in a format the platform can understand. For more information on how to use this script, run python evaluation/validate_rollout.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376678,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "7969cd91"
   },
   "outputs": [],
   "source": [
    "score = main_validation(results_path=f\"sample_results_steps_{STEP_LIMIT}_iter_{ITERS}_dl_dims_{'_'.join([str(x) for x in CUSTOM_MODEL_CONFIG['dense_layer_dims']])}.txt\", public_seeds=PUBLIC_SEEDS)\n",
    "print(f\"You scored {score}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376678,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "LVv4y1WoShkG"
   },
   "outputs": [],
   "source": [
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376678,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "p5oSzD74QVlh"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376679,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "oF5Rp-djS-Tq"
   },
   "outputs": [],
   "source": [
    "#files.download(f\"sample_results_steps_{STEP_LIMIT}_iter_{ITERS}_dl_dims_{'_'.join([str(x) for x in CUSTOM_MODEL_CONFIG['dense_layer_dims']])}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1679183376679,
     "user": {
      "displayName": "Ameck Dosseh",
      "userId": "09603352587859208137"
     },
     "user_tz": -60
    },
    "id": "Tr0tjqcoTIjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwrUJlF2GkjQXEWmR4kjaj",
   "mount_file_id": "1JG0mz2taGPOjs5Hqdu2i-FD4UpBkn_OM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
